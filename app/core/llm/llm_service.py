from langchain.output_parsers import PydanticOutputParser
from langchain_openai import ChatOpenAI

from app.core.data_sources.database_handler import DatabaseHandler
from app.core.llm.models import DataAnalysisResponse
from app.core.llm.prompts import Prompts
from app.core.utils.config import Config
from app.core.utils.logger import Logger


class LLMService:
    def __init__(self, database_url: str, model_name: str = "gpt-4o-mini"):
        """
        Initialize the LLMService with a database connection and language model.

        :param database_url: The URL of the database to connect to.
        :param model_name: The name of the language model to use. Default is "gpt-4o-mini".
        """
        self.logger = Logger(self.__class__.__name__).get_logger()
        self.db_manager = DatabaseHandler(database_url)
        self.llm = ChatOpenAI(model=model_name, openai_api_key=Config.OPENAI_API_KEY)

        # Create an output parser using the Pydantic model
        self.output_parser = PydanticOutputParser(pydantic_object=DataAnalysisResponse)

    def generate_analysis_description(self):
        """
        Generate a concise analysis description based on the database schema.

        :return: A user-friendly description string generated by the LLM.
        """
        try:
            self.logger.info("Retrieving database schema...")
            raw_schema = self.db_manager.get_schema()
            self.logger.info("Schema retrieved: %d tables found.", len(raw_schema))

            formatted_schema = self.db_manager.schema_to_string(raw_schema)
            self.logger.info("Schema formatted successfully.")

            if not formatted_schema:
                self.logger.error(
                    "Schema is empty. Unable to generate analysis description."
                )
                raise ValueError(
                    "Schema could not be retrieved. Ensure the database is accessible."
                )

            prompt_template = Prompts.data_source_overview_prompt(formatted_schema)

            self.logger.info("Generating description using the language model...")
            description_prompt = prompt_template.format()
            self.logger.debug("Prompt for LLM:\n%s", description_prompt)

            response = self.llm.invoke(
                [{"role": "system", "content": description_prompt}]
            )
            self.logger.info("Description generated successfully.")

            return response.content.strip()

        except Exception as e:
            self.logger.error(
                "Error generating analysis description: %s", str(e), exc_info=True
            )
            raise

    def process_data_analysis(
        self,
        natural_language_query: str,
        db_type: str = "SQL Server",
    ):
        """
        Process the natural language query and return a structured JSON response.
        """
        raw_schema = self.db_manager.get_schema()
        self.logger.info("Schema retrieved with %d tables.", len(raw_schema))

        formatted_schema = self.db_manager.schema_to_string(raw_schema)
        self.logger.info("Formatted schema for prompt: %s", formatted_schema[:500])

        prompt_template = Prompts.data_analysis_prompt(formatted_schema, db_type)

        formatted_prompt = prompt_template.format(
            query=natural_language_query, json_schema=Prompts.JSON_SCHEMA_DESCRIPTION
        )
        self.logger.debug("Formatted prompt for data analysis:\n%s", formatted_prompt)

        response = self.llm.invoke([{"role": "system", "content": formatted_prompt}])
        self.logger.info("Received response from LLM.")

        # Parse the response using the output parser
        return self.output_parser.parse(response.content)
