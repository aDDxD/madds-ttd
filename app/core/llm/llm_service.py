import json
from typing import List

from langchain.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel

from app.core.data_sources.database_handler import DatabaseHandler
from app.core.utils.config import Config
from app.core.utils.logger import get_logger


class VisualizationItem(BaseModel):
    description: str
    sql_query: str
    visualization: str
    plotly_express_function: str


class DataAnalysisResponse(BaseModel):
    visualizations: List[VisualizationItem]


class LLMService:
    def __init__(self, database_url: str, model_name: str = "gpt-4o-mini"):
        """
        Initialize the LLMService with a database connection and language model.

        :param database_url: The URL of the database to connect to.
        :param model_name: The name of the language model to use. Default is "gpt-4o-mini".
        """
        self.logger = get_logger(self.__class__.__name__)
        self.db_manager = DatabaseHandler(database_url)
        self.llm = ChatOpenAI(model=model_name, openai_api_key=Config.OPENAI_API_KEY)

        # Create an output parser using the Pydantic model
        self.output_parser = PydanticOutputParser(pydantic_object=DataAnalysisResponse)

    def generate_analysis_description(self):
        """
        Generate a concise analysis description based on the database schema.

        :return: A user-friendly description string generated by the LLM.
        """
        try:
            self.logger.info("Starting to retrieve the database schema.")
            raw_schema = self.db_manager.get_schema()
            self.logger.info(
                "Raw schema retrieved successfully with %d tables.", len(raw_schema)
            )

            formatted_schema = self.db_manager.schema_to_string(raw_schema)
            self.logger.info("Formatted schema generated successfully.")

            if not formatted_schema:
                self.logger.error(
                    "Failed to generate analysis description: Schema is empty."
                )
                raise ValueError(
                    "Schema could not be retrieved. Ensure the database is accessible."
                )

            # Template for a more concise, user-friendly description
            prompt_template = ChatPromptTemplate.from_template(
                template=(
                    f"You are connected to a database with the following schema:\n{formatted_schema}\n"
                    "Provide a brief, end-user-friendly description of this database, "
                    "such as 'You are connected to the XYZ database, which holds information about ABC.'"
                )
            )

            self.logger.info("Prompt template created successfully.")

            description_prompt = prompt_template.format()
            self.logger.debug("Formatted description prompt:\n%s", description_prompt)

            response = self.llm.invoke(
                [{"role": "system", "content": description_prompt}]
            )
            self.logger.info("Response from LLM received successfully.")

            return response.content.strip()

        except Exception as e:
            self.logger.error(
                f"Error generating analysis description: {str(e)}", exc_info=True
            )
            raise

    def process_data_analysis(
        self,
        natural_language_query: str,
        db_type: str = "SQL Server",
    ):
        """
        Process the natural language query and return a structured JSON response.
        """
        raw_schema = self.db_manager.get_schema()
        self.logger.info("Schema retrieved with %d tables.", len(raw_schema))

        formatted_schema = self.db_manager.schema_to_string(raw_schema)
        self.logger.info("Formatted schema generated for prompt.")

        # Log schema summary instead of the whole schema
        self.logger.info("Schema contains %d tables.", len(raw_schema))
        for table_name in list(raw_schema.keys())[:5]:  # Log first 5 table names
            self.logger.info("Table: %s", table_name)
        if len(raw_schema) > 5:
            self.logger.info("...and %d more tables.", len(raw_schema) - 5)

        prompt_template = ChatPromptTemplate.from_template(
            template=(
                f"You are a professional Data Analyst with expertise in {db_type} databases. "
                f"Given the following database schema: {formatted_schema} "
                f"your task is to generate meaningful insights from the natural language query: '{{{{query}}}}'. "
                f"Provide as many visualizations as necessary to comprehensively address the query. "
                f"Your response must be structured as a JSON object adhering to the following schema: "
                "{json_schema} "
                f"Each item in the 'visualizations' array should include: "
                f"- 'description': A brief explanation of the suggested data visualization and its purpose. "
                f"- 'sql_query': A valid SQL query that must be compatible with the {db_type} database. Ensure the query only references columns available in the provided schema. "
                f"- 'visualization': Suggested visualization type (e.g., bar, line, pie, etc.). "
                f"- 'plotly_express_function': A complete Plotly Express function call to generate the suggested visualization, e.g., 'px.bar(data, x=\"column_name\", y=\"column_name\")'. "
                "Your analysis should be both accurate and actionable, helping to uncover key trends, comparisons, and insights from the data."
            )
        )
        schema_description = "{'visualizations': [{'description': 'string', 'sql_query': 'string', 'visualization': 'string', 'plotly_express_function': 'string'}]}"

        formatted_prompt = prompt_template.format(
            query=natural_language_query, json_schema=schema_description
        )
        self.logger.debug("Formatted data analysis prompt:\n%s", formatted_prompt)

        response = self.llm.invoke([{"role": "system", "content": formatted_prompt}])

        # Parse the response using the output parser
        return self.output_parser.parse(response.content)
