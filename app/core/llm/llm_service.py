import re

from langchain_openai import ChatOpenAI

from app.core.data_sources.data_source import DataSource
from app.core.llm.chroma_service import ChromaService
from app.core.llm.prompts import Prompts
from app.core.utils.config import Config
from app.core.utils.logger import Logger


class LLMService:
    def __init__(self, source: str, model_name: str = "gpt-4o-mini"):
        self.logger = Logger(self.__class__.__name__).get_logger()
        self.data_source = DataSource.create(source)
        self.llm = ChatOpenAI(model=model_name, openai_api_key=Config().OPENAI_API_KEY)
        self.chroma_service = ChromaService()
        self._initialize_chroma_db()

    def _initialize_chroma_db(self):
        existing_docs = self.chroma_service.collection.get()

        if not existing_docs or len(existing_docs.get("documents", [])) == 0:
            self.logger.info("Vectorizing schema and storing in ChromaDB for RAG...")
            raw_schema = self.data_source.get_schema()
            formatted_schema = self.data_source.schema_to_string(raw_schema)

            schema_chunks = formatted_schema.split("\n\n")
            self.chroma_service.add_schema_vectors(schema_chunks)
            self.logger.info("Schema stored in ChromaDB successfully.")
        else:
            self.logger.info("Schema data already available in ChromaDB.")

    def generate_analysis_description(self):
        self.logger.info("Retrieving schema description using RAG...")

        chroma_results = self.chroma_service.query_schema(
            "key data source details to enable complex data analysis"
        )

        if not chroma_results:
            self.logger.error("No schema data found in ChromaDB.")
            raise ValueError("No schema data available for analysis.")

        relevant_schema_texts = [doc["text"] for doc in chroma_results]

        if not relevant_schema_texts:
            self.logger.error("No valid schema text found in ChromaDB results.")
            raise ValueError("No valid schema text found in ChromaDB results.")

        relevant_schema_text = "\n\n".join(relevant_schema_texts)

        messages = Prompts.data_source_overview_prompt(relevant_schema_text)

        response = self.llm.invoke(messages)

        self.logger.info(f"Analysis description generated successfully.")

        return response.content.strip()

    def process_data_analysis(
        self, natural_language_query: str, db_type: str = "SQL Server"
    ) -> str:
        self.logger.info("Retrieving schema for analysis using RAG...")

        chroma_results = self.chroma_service.query_schema(natural_language_query)

        if not chroma_results:
            self.logger.error("No relevant schema found in ChromaDB for analysis.")
            raise ValueError("No relevant schema found in ChromaDB.")

        relevant_schema_text = "\n\n".join([doc["text"] for doc in chroma_results])

        # Prepare the structured prompt using the new format
        messages = Prompts.dashboard_creation_prompt(
            formatted_schema=relevant_schema_text,
            db_type=db_type,
            query=natural_language_query,
        )

        # Invoke the LLM with the structured multi-message prompt
        dashboard_response = self.llm.invoke(messages)

        self.logger.info("Dashboard code generated by LLM successfully.")

        final_code = dashboard_response.content.strip()

        # Clean up the code from any unnecessary formatting
        final_code = re.sub(r"```(?:python)?\n", "", final_code)
        final_code = re.sub(r"```", "", final_code)

        return final_code
